{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cvxopt import solvers, matrix, spmatrix, spdiag, sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from softsvm import softsvm, fix_small_eigvals, error, get_random_sample\n",
    "import softsvm\n",
    "\n",
    "data = np.load('ex2q4_data.npz')\n",
    "trainX, testX = data['Xtrain'], data['Xtest']\n",
    "trainY, testY = data['Ytrain'], data['Ytest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K(xi, xj, k):\n",
    "    \"\"\"\n",
    "    :param xi, xj: two vectors of the same dimension\n",
    "    :param k: the degree of the poly kernel\n",
    "    :return: (1 + dot_res)^k\n",
    "    \"\"\"\n",
    "    # sanity check\n",
    "    assert xi.shape == xj.shape, \"xi and xj should be of the same shape\\ngot xi.shape = {}, xj.shape = {}\".format(xi.shape, xj.shape)\n",
    "    return np.power(np.dot(xi, xj) + 1.0, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram_matrix(X, k):\n",
    "    m = X.shape[0]\n",
    "    G = np.zeros((m, m))\n",
    "    for row in range(m):\n",
    "        for col in range(row ,m):\n",
    "            G[col, row] = G[row, col] = K(X[row], X[col], k)\n",
    "    return G\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softsvmpoly(l: float, k: float, trainX: np.array, trainy: np.array):\n",
    "    \"\"\"\n",
    "    :param l: the parameter lambda of the soft SVM algorithm\n",
    "    :param sigma: the bandwidth parameter sigma of the RBF kernel.\n",
    "    :param trainX: numpy array of size (m, d) containing the training sample\n",
    "    :param trainy: numpy array of size (m, 1) containing the labels of the training sample\n",
    "    :return: numpy array of size (m, 1) which describes the coefficients found by the algorithm\n",
    "    \"\"\"\n",
    "    m, d = trainX.shape\n",
    "    \n",
    "    G = get_gram_matrix(trainX, k)\n",
    "\n",
    "    H = np.pad(float(2 * l) * G, [(0, m), (0, m)])\n",
    "    H = fix_small_eigvals(H)\n",
    "\n",
    "    A = np.block([[np.zeros((m, m)), np.identity(m)],\n",
    "                  [G * trainy.reshape(-1, 1), np.identity(m)]])\n",
    "\n",
    "    u = np.hstack((np.full(m, float(0)), np.full(m, 1/m)))\n",
    "\n",
    "    v = np.hstack((np.zeros(m), np.ones(m)))\n",
    "  \n",
    "    z = solvers.qp(matrix(H), matrix(u), -matrix(A), -matrix(v))\n",
    "    alphas = np.array(z[\"x\"])[:m]\n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_product(set_a: np.array, set_b: np.array):\n",
    "    return [(ai, bi) for ai in set_a for bi in set_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_sample(alphas: np.array, k: int, sample: np.array, trainX: np.array):\n",
    "    train_kernels = np.array([K(sample, xi, k) for xi in trainX])\n",
    "    return np.sign(np.dot(train_kernels, alphas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(alphas: np.array, k: int, testX: np.array, trainX: np.array):\n",
    "    \"\"\"\n",
    "    :param alphas: numpy array of size (m, 1) containing the coefficients of the soft SVM algorithm\n",
    "    :param testX: numpy array of size (m, d) containing the test sample\n",
    "    :param k: int, the degree of the poly kernel\n",
    "    :return: numpy array of size (m, 1) containing the predicted labels of the test sample\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.array([predict_single_sample(alphas, k, sample, trainX) for sample in testX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(folds: int):\n",
    "    X_chunks = np.array(np.split(trainX, folds))\n",
    "    Y_chunks = np.array(np.split(trainY, folds))\n",
    "    splitted = []\n",
    "    shp = X_chunks.shape\n",
    "    for i in range(folds):\n",
    "        test, test_labales = X_chunks[i], Y_chunks[i]\n",
    "        # current test set is the the entire train besides the current chunk used for training\n",
    "        train = np.concatenate(np.delete(X_chunks, i, axis=0))\n",
    "        train_labales = np.concatenate(np.delete(Y_chunks, i, axis=0))\n",
    "        splitted.append({\"train\": train,\n",
    "                        \"train_labales\": train_labales,\n",
    "                        \"test\": test,\n",
    "                        \"test_labales\": test_labales})\n",
    "    return np.array(splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_l_k(dict, source):\n",
    "    with open (\"cross validation sorted results.txt\", \"a+\") as f:\n",
    "        f.write(source + \"\\n\")\n",
    "        for k, v in sorted(dict.items(), key=lambda x: x[1]):\n",
    "            f.write(\"{}: {}\\n\".format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_cross_validation(lambdas: np.array, ks, folds: int):\n",
    "    \"\"\"\n",
    "    find pair (lambda, k) with the lowest validation error, and get classifier based on that pair\n",
    "    :param lambdas: the lambda parameters to use\n",
    "    :param ks: the k parameters to use in the kernel function\n",
    "    :param folds: number of chunks we split the data to\n",
    "    :return: prediction of test set of classifier trained on the entire train set using best lambda and k found\n",
    "    \"\"\"\n",
    "    # poly softSVM\n",
    "    errors = {(l, k): 0 for l, k in cartesian_product(lambdas, ks)}\n",
    "    a = split_data(folds)\n",
    "    for fold in split_data(folds):\n",
    "        for l, k in cartesian_product(lambdas, ks):\n",
    "            print(l,k)\n",
    "            alphas = softsvmpoly(l, k, fold[\"train\"], fold[\"train_labales\"])\n",
    "            predicted = predict(alphas, k, fold[\"test\"], fold[\"train\"])\n",
    "            # continuesly calculate the avg error\n",
    "            errors[(l, k)] += error(fold[\"test_labales\"], predicted) / folds\n",
    "    \n",
    "    # get the pair with lowest avg error\n",
    "    best_lambda, best_k = min(errors.items(), key=lambda x: x[1])[0]\n",
    "    log_l_k(errors, \"poly kernel softsvm errors by (lambda, k):\")\n",
    "    print(f\"best_lambda={best_lambda}, best_k={best_k}\")\n",
    "    alphas = softsvmpoly(best_lambda, best_k, trainX, trainY)\n",
    "    return predict(alphas, best_k, testX, trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softsvm_cross_validation(lambdas, folds):\n",
    "    errors = {l: 0 for l in lambdas}\n",
    "    for fold in split_data(folds):\n",
    "        for l in lambdas:\n",
    "            w = softsvm.softsvm(l, fold[\"train\"], fold[\"train_labales\"])\n",
    "            errors[l] += error(fold[\"test_labales\"], softsvm.predict(w, fold[\"test\"])) / folds\n",
    "    \n",
    "    # get the l with lowest avg error\n",
    "    best_lambda = min(errors.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    log_l_k(errors, \"linear softsvm errors by (lambda, k):\")\n",
    "    print(f\"best_lambda={best_lambda}\")\n",
    "    w = softsvm.softsvm(best_lambda, trainX, trainY)\n",
    "    return softsvm.predict(w, testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_error():\n",
    "    lambdas = np.array([1.0, 10.0, 100.0])\n",
    "    ks = np.array([2.0, 5.0, 8.0])\n",
    "    k_folds = 5\n",
    "    # polynomial kernel\n",
    "    poly_predicions = poly_cross_validation(lambdas, ks, folds=k_folds)\n",
    "    poly_error = error(testY, poly_predicions)\n",
    "    # best_lambda=10.0, best_k=5.0\n",
    "\n",
    "    # linear softsvm\n",
    "    soft_svm_predictions = softsvm_cross_validation(lambdas, folds=k_folds)\n",
    "    soft_svm_error =  error(testY, soft_svm_predictions)\n",
    "    # best_lambda=1.0\n",
    "\n",
    "    print(f\"soft_svm_error={soft_svm_error}\\t poly_error={poly_error}\\n {soft_svm_error > poly_error}\")\n",
    "# cross_validation_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q4_e():\n",
    "    def plot_predictor(l, k, alphas, ax):\n",
    "        step_size = 0.02\n",
    "        x = np.arange(trainX[:, 0].min(), trainX[:, 0].max(), step_size)\n",
    "        y = np.arange(trainX[:, 1].min(), trainX[:, 1].max(), step_size)\n",
    "\n",
    "        grid = [[predict_single_sample(alphas, k, np.array([xi, yi]), trainX) for xi in x] for yi in reversed(y)]   \n",
    "        ax.imshow(grid, cmap='coolwarm', extent=[-1, 1, 1, -1])\n",
    "        ax.set_title(f\"λ={int(l)} k={int(k)}\")\n",
    "\n",
    "    l = 100.0\n",
    "    ks = np.array([3.0, 5.0, 8.0])\n",
    "\n",
    "    # Create a figure with 3 subplots\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
    "    # set space between subplots\n",
    "    fig.subplots_adjust(wspace=0.5)\n",
    "\n",
    "    for ax, k in zip(axs, ks):\n",
    "        alphas = softsvmpoly(l, k, trainX, trainY)\n",
    "        plot_predictor(l ,k, alphas, ax)\n",
    "    plt.savefig(\"Q4_e.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Q4_e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "def Q4_e_threaded():\n",
    "    def plot_predictor(l, k, alphas, ax):\n",
    "        step_size = 0.02\n",
    "        x = np.arange(trainX[:, 0].min(), trainX[:, 0].max(), step_size)\n",
    "        y = np.arange(trainX[:, 1].min(), trainX[:, 1].max(), step_size)\n",
    "\n",
    "        grid = [[predict_single_sample(alphas, k, np.array([xi, yi]), trainX) for xi in x] for yi in reversed(y)]   \n",
    "        ax.imshow(grid, cmap='coolwarm', extent=[-1, 1, 1, -1])\n",
    "        ax.set_title(f\"λ={int(l)} k={int(k)}\")\n",
    "\n",
    "    l = 100.0\n",
    "    ks = np.array([3.0, 5.0, 8.0])\n",
    "    \n",
    "    # Create a figure with 3 subplots\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
    "    # set space between subplots\n",
    "    fig.subplots_adjust(wspace=0.5)\n",
    "\n",
    "    # plot in parallel\n",
    "    threads = []\n",
    "    for ax, k in zip(axs, ks):\n",
    "        t = threading.Thread(target=plot_predictor, args=(l, k, softsvmpoly(l, k, trainX, trainY), ax))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    # make sure all threads are done before saving the figure\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    \n",
    "    plt.savefig(\"Q4_e.png\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  5.1453e+00  8.3494e+01  5e+03  2e+00  2e+05\n",
      " 1:  1.7230e+01 -1.9380e+02  2e+02  9e-02  6e+03\n",
      " 2:  1.2168e+01 -1.9752e+01  3e+01  8e-03  6e+02\n",
      " 3:  7.1201e+00 -5.2189e+00  1e+01  3e-03  2e+02\n",
      " 4:  2.9410e+00  1.2945e+00  2e+00  2e-15  4e-13\n",
      " 5:  2.1432e+00  1.8659e+00  3e-01  1e-15  3e-13\n",
      " 6:  2.0171e+00  1.9725e+00  4e-02  1e-15  5e-13\n",
      " 7:  1.9960e+00  1.9882e+00  8e-03  1e-15  2e-12\n",
      " 8:  1.9922e+00  1.9909e+00  1e-03  1e-15  1e-12\n",
      " 9:  1.9916e+00  1.9914e+00  2e-04  1e-15  2e-12\n",
      "10:  1.9915e+00  1.9915e+00  8e-06  1e-15  6e-12\n",
      "11:  1.9915e+00  1.9915e+00  2e-07  1e-15  1e-10\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "def B(k, t):\n",
    "    \"\"\" multinmumial coefficient \"\"\"\n",
    "    denominator = np.prod([np.math.factorial(i) for i in t])\n",
    "    numerator = np.math.factorial(k)\n",
    "    return numerator / denominator\n",
    "\n",
    "def psi(x, k):\n",
    "    psi = []\n",
    "    for i in np.arange(k + 1):\n",
    "        for j in np.arange(k + 1):\n",
    "            if i + j <= k:\n",
    "                t = np.array((i, j))\n",
    "                psi.append(np.sqrt(B(k ,t)) * np.prod(np.power(x, t)))\n",
    "    return np.array(psi) \n",
    "    \n",
    "def Q4_f():\n",
    "    m, d = trainX.shape\n",
    "    l, k = 1.0, 5.0\n",
    "    alphas = softsvmpoly(l, k, trainX, trainY)\n",
    "    w = np.array([alphas[i] * psi(trainX[i], k) for i in range(m)]).sum(axis=0)\n",
    "    return w\n",
    "    \n",
    "# Q4_f()\n",
    "w = Q4_f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Figure' object has no attribute 'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3416\\214793556.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"w predictions on trainig and testing set\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m scatter = ax.scatter(x=merged_space[:, 0], y=merged_space[:, 1],\n\u001b[0;32m     14\u001b[0m                     c=prediction, cmap='coolwarm',alpha=0.75)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Figure' object has no attribute 'title'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_space = np.vstack((trainX, testX))\n",
    "axis = np.split(merged_space, 2, axis=1)\n",
    "new_space = []\n",
    "l, k = 1.0, 5.0\n",
    "for i, sample in enumerate(merged_space):\n",
    "    new_space.append(psi(sample, k))\n",
    "new_space = np.array(new_space)\n",
    "\n",
    "prediction = softsvm.predict(w, new_space)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.set_title(\"w predictions on trainig and testing set\")\n",
    "scatter = ax.scatter(x=merged_space[:, 0], y=merged_space[:, 1],\n",
    "                    c=prediction, cmap='coolwarm',alpha=0.75)\n",
    "\n",
    "# Create the colorbar\n",
    "colorbar = plt.colorbar(scatter, ax=ax)\n",
    "colorbar.set_label('Colorbar Label')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08ed5fa07ca9cd55beeef47f224bbbd5867169d4ebe490c55adacd701e94c428"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
